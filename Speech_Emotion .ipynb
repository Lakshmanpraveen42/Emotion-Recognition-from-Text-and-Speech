{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Speech Emotion Recognition"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-22T01:12:18.883272Z","iopub.status.busy":"2024-05-22T01:12:18.882434Z","iopub.status.idle":"2024-05-22T01:12:18.890669Z","shell.execute_reply":"2024-05-22T01:12:18.889664Z","shell.execute_reply.started":"2024-05-22T01:12:18.883237Z"},"trusted":true},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import tensorflow\n","import os\n","import sys\n","\n","# librosa is a Python library for analyzing audio and music. It can be used to extract the data from the audio files we will see it later.\n","import librosa\n","import librosa.display\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","\n","from sklearn.preprocessing import StandardScaler, OneHotEncoder\n","from sklearn.metrics import confusion_matrix, classification_report\n","from sklearn.model_selection import train_test_split\n","\n","# to play the audio files\n","from IPython.display import Audio\n","\n","import keras\n","from keras.callbacks import ReduceLROnPlateau\n","from keras.models import Sequential\n","from keras.layers import Dense, Conv1D, MaxPooling1D, Flatten, Dropout, BatchNormalization\n","from tensorflow.keras.utils import to_categorical\n","\n","from keras.callbacks import ModelCheckpoint\n","\n","import warnings\n","if not sys.warnoptions:\n","    warnings.simplefilter(\"ignore\")\n","warnings.filterwarnings(\"ignore\", category=DeprecationWarning)"]},{"cell_type":"markdown","metadata":{},"source":["## Data Preparation"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-22T01:12:26.162057Z","iopub.status.busy":"2024-05-22T01:12:26.161106Z","iopub.status.idle":"2024-05-22T01:12:26.165786Z","shell.execute_reply":"2024-05-22T01:12:26.164954Z","shell.execute_reply.started":"2024-05-22T01:12:26.162015Z"},"trusted":true},"outputs":[],"source":["Savee = \"/kaggle/input/surrey-audiovisual-expressed-emotion-savee/ALL\"\n","Ravdess = \"/kaggle/input/ravdess-emotional-speech-audio/audio_speech_actors_01-24/\""]},{"cell_type":"markdown","metadata":{},"source":["##  <center> 1. Ravdess Dataframe <center>"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-22T01:12:29.102746Z","iopub.status.busy":"2024-05-22T01:12:29.102398Z","iopub.status.idle":"2024-05-22T01:12:29.148641Z","shell.execute_reply":"2024-05-22T01:12:29.147769Z","shell.execute_reply.started":"2024-05-22T01:12:29.102717Z"},"trusted":true},"outputs":[],"source":["ravdess_directory_list = os.listdir(Ravdess)\n","\n","file_emotion = []\n","file_path = []\n","for dir in ravdess_directory_list:\n","    # as their are 20 different actors in our previous directory we need to extract files for each actor.\n","    actor = os.listdir(Ravdess + dir)\n","    for file in actor:\n","        part = file.split('.')[0]\n","        part = part.split('-')\n","        # third part in each file represents the emotion associated to that file.\n","        file_emotion.append(int(part[2]))\n","        file_path.append(Ravdess + dir + '/' + file)\n","\n","# dataframe for emotion of files\n","emotion_df = pd.DataFrame(file_emotion, columns=['Emotions'])\n","\n","# dataframe for path of files.\n","path_df = pd.DataFrame(file_path, columns=['Path'])\n","Ravdess_df = pd.concat([emotion_df, path_df], axis=1)\n","\n","# changing integers to actual emotions.\n","Ravdess_df.Emotions.replace({1:'neutral', 2:'calm', 3:'happy', 4:'sad', 5:'angry', 6:'fear', 7:'disgust', 8:'surprise'}, inplace=True)\n","Ravdess_df.head()"]},{"cell_type":"markdown","metadata":{},"source":["##  <center> 2. CREMA-D dataset <center>\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-22T01:12:30.173311Z","iopub.status.busy":"2024-05-22T01:12:30.172329Z","iopub.status.idle":"2024-05-22T01:12:30.189744Z","shell.execute_reply":"2024-05-22T01:12:30.188801Z","shell.execute_reply.started":"2024-05-22T01:12:30.173277Z"},"trusted":true},"outputs":[],"source":["savee_directory_list = os.listdir(Savee)\n","\n","file_emotion = []\n","file_path = []\n","\n","for file in savee_directory_list:\n","    file_path.append(Savee + file)\n","    part = file.split('_')[1]\n","    ele = part[:-6]\n","    if ele=='a':\n","        file_emotion.append('angry')\n","    elif ele=='d':\n","        file_emotion.append('disgust')\n","    elif ele=='f':\n","        file_emotion.append('fear')\n","    elif ele=='h':\n","        file_emotion.append('happy')\n","    elif ele=='n':\n","        file_emotion.append('neutral')\n","    elif ele=='sa':\n","        file_emotion.append('sad')\n","    else:\n","        file_emotion.append('surprise')\n","\n","# dataframe for emotion of files\n","emotion_df = pd.DataFrame(file_emotion, columns=['Emotions'])\n","\n","# dataframe for path of files.\n","path_df = pd.DataFrame(file_path, columns=['Path'])\n","Savee_df = pd.concat([emotion_df, path_df], axis=1)\n","Savee_df.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-22T01:12:30.641792Z","iopub.status.busy":"2024-05-22T01:12:30.641184Z","iopub.status.idle":"2024-05-22T01:12:30.662965Z","shell.execute_reply":"2024-05-22T01:12:30.661948Z","shell.execute_reply.started":"2024-05-22T01:12:30.641760Z"},"trusted":true},"outputs":[],"source":["# creating Dataframe using all the 4 dataframes we created so far.\n","data_path = pd.concat([ Ravdess_df,Savee_df], axis = 0)\n","data_path.to_csv(\"data_path.csv\",index=False)\n","data_path.head()\n"]},{"cell_type":"markdown","metadata":{},"source":["## Data Visualisation and Exploration"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-22T01:12:31.902633Z","iopub.status.busy":"2024-05-22T01:12:31.902277Z","iopub.status.idle":"2024-05-22T01:12:32.233061Z","shell.execute_reply":"2024-05-22T01:12:32.232236Z","shell.execute_reply.started":"2024-05-22T01:12:31.902606Z"},"trusted":true},"outputs":[],"source":["\n","\n","# Plotting the count of emotions\n","plt.figure(figsize=(10, 6))\n","plt.title('Count of Emotions', fontsize=16)\n","sns.countplot(x='Emotions', data=data_path, palette='viridis')\n","plt.ylabel('Count', fontsize=12)\n","plt.xlabel('Emotions', fontsize=12)\n","sns.despine(top=True, right=True, left=False, bottom=False)\n","plt.xticks(rotation=45)  # Rotate x-axis labels if they overlap\n","plt.tight_layout()  # Adjust subplots to fit into figure area.\n","plt.show()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-22T01:12:33.291839Z","iopub.status.busy":"2024-05-22T01:12:33.291179Z","iopub.status.idle":"2024-05-22T01:12:34.080742Z","shell.execute_reply":"2024-05-22T01:12:34.079872Z","shell.execute_reply.started":"2024-05-22T01:12:33.291806Z"},"trusted":true},"outputs":[],"source":["\n","\n","# Function to create waveplot\n","def create_waveplot(data, sr, e):\n","    plt.figure(figsize=(10, 3))\n","    plt.title(f'Waveplot for audio with {e} emotion', size=15)\n","    librosa.display.waveshow(data, sr=sr)\n","    plt.show()\n","\n","# Function to create spectrogram (assuming you have this function defined elsewhere)\n","def create_spectrogram(data, sr, e):\n","    plt.figure(figsize=(10, 3))\n","    plt.specgram(data, Fs=sr)\n","    plt.title(f'Spectrogram for audio with {e} emotion', size=15)\n","    plt.xlabel('Time')\n","    plt.ylabel('Frequency')\n","    plt.show()\n","\n","# Example usage\n","emotion='fear'\n","path = np.array(data_path.Path[data_path.Emotions==emotion])[1]\n","data, sampling_rate = librosa.load(path)\n","create_waveplot(data, sampling_rate, emotion)\n","create_spectrogram(data, sampling_rate, emotion)\n","Audio(path)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-22T01:12:34.652944Z","iopub.status.busy":"2024-05-22T01:12:34.652319Z","iopub.status.idle":"2024-05-22T01:12:35.432743Z","shell.execute_reply":"2024-05-22T01:12:35.431842Z","shell.execute_reply.started":"2024-05-22T01:12:34.652892Z"},"trusted":true},"outputs":[],"source":["emotion='angry'\n","path = np.array(data_path.Path[data_path.Emotions==emotion])[1]\n","data, sampling_rate = librosa.load(path)\n","create_waveplot(data, sampling_rate, emotion)\n","create_spectrogram(data, sampling_rate, emotion)\n","Audio(path)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-22T01:12:35.434904Z","iopub.status.busy":"2024-05-22T01:12:35.434516Z","iopub.status.idle":"2024-05-22T01:12:36.202513Z","shell.execute_reply":"2024-05-22T01:12:36.201612Z","shell.execute_reply.started":"2024-05-22T01:12:35.434874Z"},"trusted":true},"outputs":[],"source":["emotion='sad'\n","path = np.array(data_path.Path[data_path.Emotions==emotion])[1]\n","data, sampling_rate = librosa.load(path)\n","create_waveplot(data, sampling_rate, emotion)\n","create_spectrogram(data, sampling_rate, emotion)\n","Audio(path)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-22T01:12:36.204614Z","iopub.status.busy":"2024-05-22T01:12:36.204300Z","iopub.status.idle":"2024-05-22T01:12:36.983137Z","shell.execute_reply":"2024-05-22T01:12:36.982275Z","shell.execute_reply.started":"2024-05-22T01:12:36.204587Z"},"trusted":true},"outputs":[],"source":["emotion='happy'\n","path = np.array(data_path.Path[data_path.Emotions==emotion])[1]\n","data, sampling_rate = librosa.load(path)\n","create_waveplot(data, sampling_rate, emotion)\n","create_spectrogram(data, sampling_rate, emotion)\n","Audio(path)"]},{"cell_type":"markdown","metadata":{},"source":["## Data Augmentation\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-22T01:12:36.984694Z","iopub.status.busy":"2024-05-22T01:12:36.984381Z","iopub.status.idle":"2024-05-22T01:12:36.993386Z","shell.execute_reply":"2024-05-22T01:12:36.992415Z","shell.execute_reply.started":"2024-05-22T01:12:36.984668Z"},"trusted":true},"outputs":[],"source":["# Define augmentation functions\n","def noise(data):\n","    noise_amp = 0.035 * np.random.uniform() * np.amax(data)\n","    data = data + noise_amp * np.random.normal(size=data.shape[0])\n","    return data\n","\n","def stretch(data, rate=0.8):\n","    return librosa.effects.time_stretch(data, rate=rate)\n","\n","def shift(data):\n","    shift_range = int(np.random.uniform(low=-5, high=5) * 1000)\n","    return np.roll(data, shift_range)\n","\n","def pitch(data, sampling_rate, pitch_factor=0.7):\n","    return librosa.effects.pitch_shift(data, sr=sampling_rate, n_steps=pitch_factor)"]},{"cell_type":"markdown","metadata":{},"source":["#### 1. Simple Audio"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-22T01:12:37.186824Z","iopub.status.busy":"2024-05-22T01:12:37.186462Z","iopub.status.idle":"2024-05-22T01:12:37.572469Z","shell.execute_reply":"2024-05-22T01:12:37.571627Z","shell.execute_reply.started":"2024-05-22T01:12:37.186795Z"},"trusted":true},"outputs":[],"source":["\n","#plt.figure(figsize=(14,4))\n","#librosa.display.waveshow(y=data, sr=sampling_rate)\n","#Audio(path)\n","\n","plt.figure(figsize=(14, 4))\n","librosa.display.waveshow(y=data, sr=sampling_rate)\n","plt.title('Waveform')\n","plt.xlabel('Time (s)')\n","plt.ylabel('Amplitude')\n","plt.show()\n","\n","# Play the audio\n","Audio(path)"]},{"cell_type":"markdown","metadata":{},"source":["#### 2. Noise Injection"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-22T01:12:40.016377Z","iopub.status.busy":"2024-05-22T01:12:40.016013Z","iopub.status.idle":"2024-05-22T01:12:40.525616Z","shell.execute_reply":"2024-05-22T01:12:40.524677Z","shell.execute_reply.started":"2024-05-22T01:12:40.016349Z"},"trusted":true},"outputs":[],"source":["x = noise(data)\n","plt.figure(figsize=(14,4))\n","librosa.display.waveshow(y=x, sr=sampling_rate)\n","Audio(x, rate=sampling_rate)"]},{"cell_type":"markdown","metadata":{},"source":["#### 3. Stretching"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-22T01:12:42.548114Z","iopub.status.busy":"2024-05-22T01:12:42.547415Z","iopub.status.idle":"2024-05-22T01:12:43.078816Z","shell.execute_reply":"2024-05-22T01:12:43.077947Z","shell.execute_reply.started":"2024-05-22T01:12:42.548080Z"},"trusted":true},"outputs":[],"source":["import librosa.effects\n","\n","# Stretch the audio\n","x = librosa.effects.time_stretch(data,rate=1.5)\n","plt.figure(figsize=(14,4))\n","librosa.display.waveshow(y=x, sr=sampling_rate)\n","Audio(x, rate=sampling_rate)"]},{"cell_type":"markdown","metadata":{},"source":["#### 4. Shifting"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-22T01:12:43.207359Z","iopub.status.busy":"2024-05-22T01:12:43.206839Z","iopub.status.idle":"2024-05-22T01:12:43.697936Z","shell.execute_reply":"2024-05-22T01:12:43.697025Z","shell.execute_reply.started":"2024-05-22T01:12:43.207332Z"},"trusted":true},"outputs":[],"source":["x = shift(data)\n","plt.figure(figsize=(14,4))\n","librosa.display.waveshow(y=x, sr=sampling_rate)\n","Audio(x, rate=sampling_rate)"]},{"cell_type":"markdown","metadata":{},"source":["#### 5. Pitch"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-22T01:12:44.063894Z","iopub.status.busy":"2024-05-22T01:12:44.063106Z","iopub.status.idle":"2024-05-22T01:12:44.603743Z","shell.execute_reply":"2024-05-22T01:12:44.602848Z","shell.execute_reply.started":"2024-05-22T01:12:44.063862Z"},"trusted":true},"outputs":[],"source":["x = librosa.effects.pitch_shift(data,sr=sampling_rate,n_steps=7)\n","plt.figure(figsize=(14,4))\n","librosa.display.waveshow(y=x, sr=sampling_rate)\n","Audio(x, rate=sampling_rate)"]},{"cell_type":"markdown","metadata":{},"source":[]},{"cell_type":"markdown","metadata":{},"source":["\n","## Feature Extraction\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-22T01:12:45.074800Z","iopub.status.busy":"2024-05-22T01:12:45.074173Z","iopub.status.idle":"2024-05-22T01:12:45.083306Z","shell.execute_reply":"2024-05-22T01:12:45.082211Z","shell.execute_reply.started":"2024-05-22T01:12:45.074769Z"},"trusted":true},"outputs":[],"source":["def extract_features(data, sampling_rate):\n","    # ZCR\n","    result = np.array([])\n","    zcr = np.mean(librosa.feature.zero_crossing_rate(y=data).T, axis=0)\n","    result = np.hstack((result, zcr))  # stacking horizontally\n","\n","    # Chroma_stft\n","    stft = np.abs(librosa.stft(data))\n","    chroma_stft = np.mean(librosa.feature.chroma_stft(S=stft, sr=sampling_rate).T, axis=0)\n","    result = np.hstack((result, chroma_stft))  # stacking horizontally\n","\n","    # MFCC\n","    mfcc = np.mean(librosa.feature.mfcc(y=data, sr=sampling_rate, n_mfcc=13).T, axis=0)\n","    result = np.hstack((result, mfcc))  # stacking horizontally\n","\n","    # Root Mean Square Value\n","    rms = np.mean(librosa.feature.rms(y=data).T, axis=0)\n","    result = np.hstack((result, rms))  # stacking horizontally\n","\n","    # MelSpectrogram\n","    mel = np.mean(librosa.feature.melspectrogram(y=data, sr=sampling_rate, n_mels=128).T, axis=0)\n","    result = np.hstack((result, mel))  # stacking horizontally\n","\n","    return result\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-22T01:12:47.982568Z","iopub.status.busy":"2024-05-22T01:12:47.982224Z","iopub.status.idle":"2024-05-22T01:12:47.989780Z","shell.execute_reply":"2024-05-22T01:12:47.988839Z","shell.execute_reply.started":"2024-05-22T01:12:47.982538Z"},"trusted":true},"outputs":[],"source":["def get_features(path):\n","    try:\n","        # duration and offset are used to take care of the no audio in start and the ending of each audio files as seen above.\n","        data, sampling_rate = librosa.load(path, duration=2.5, offset=0.6)\n","\n","        # without augmentation\n","        res1 = extract_features(data, sampling_rate)\n","        result = np.array(res1)\n","\n","        # data with noise\n","        noise_data = noise(data)\n","        res2 = extract_features(noise_data, sampling_rate)\n","        result = np.vstack((result, res2))  # stacking vertically\n","\n","        # data with stretching and pitching\n","        new_data = stretch(data)\n","        data_stretch_pitch = pitch(new_data, sampling_rate)\n","        res3 = extract_features(data_stretch_pitch, sampling_rate)\n","        result = np.vstack((result, res3))  # stacking vertically\n","\n","        return result\n","    except Exception as e:\n","        print(f\"Error processing {path}: {e}\")\n","        return None\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-22T01:12:50.242593Z","iopub.status.busy":"2024-05-22T01:12:50.242238Z","iopub.status.idle":"2024-05-22T01:20:28.071014Z","shell.execute_reply":"2024-05-22T01:20:28.069775Z","shell.execute_reply.started":"2024-05-22T01:12:50.242564Z"},"trusted":true},"outputs":[],"source":["X, Y = [], []\n","for path, emotion in zip(data_path.Path, data_path.Emotions):\n","    features = get_features(path)\n","    if features is not None:\n","        for feature in features:\n","            X.append(feature)\n","            # appending emotion 3 times as we have made 3 augmentation techniques on each audio file.\n","            Y.append(emotion)\n","\n","X = np.array(X)\n","Y = np.array(Y)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-22T01:24:27.623345Z","iopub.status.busy":"2024-05-22T01:24:27.622998Z","iopub.status.idle":"2024-05-22T01:24:27.629681Z","shell.execute_reply":"2024-05-22T01:24:27.628734Z","shell.execute_reply.started":"2024-05-22T01:24:27.623319Z"},"trusted":true},"outputs":[],"source":["len(X), len(Y), data_path.Path.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-22T01:24:29.410783Z","iopub.status.busy":"2024-05-22T01:24:29.410387Z","iopub.status.idle":"2024-05-22T01:24:29.415674Z","shell.execute_reply":"2024-05-22T01:24:29.414787Z","shell.execute_reply.started":"2024-05-22T01:24:29.410754Z"},"trusted":true},"outputs":[],"source":["print(f\"Features shape: {X.shape}\")\n","print(f\"Labels shape: {Y.shape}\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-22T01:24:31.286566Z","iopub.status.busy":"2024-05-22T01:24:31.286212Z","iopub.status.idle":"2024-05-22T01:24:31.388378Z","shell.execute_reply":"2024-05-22T01:24:31.387542Z","shell.execute_reply.started":"2024-05-22T01:24:31.286537Z"},"trusted":true},"outputs":[],"source":["import plotly.express as px\n","import plotly.graph_objects as go\n","from yellowbrick.features import PCA as YBPCA\n","\n","fig = px.histogram(data_path, x='Emotions', title='Count of Emotions', \n","                   labels={'Emotions':'Emotions'}, \n","                   color='Emotions', \n","                   color_discrete_sequence=px.colors.qualitative.Vivid)\n","\n","fig.update_layout(title_text='Count of Emotions', title_x=0.5, \n","                  xaxis_title='Emotions', yaxis_title='Count')\n","\n","fig.show()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-22T01:24:40.183191Z","iopub.status.busy":"2024-05-22T01:24:40.182598Z","iopub.status.idle":"2024-05-22T01:24:40.368237Z","shell.execute_reply":"2024-05-22T01:24:40.367324Z","shell.execute_reply.started":"2024-05-22T01:24:40.183163Z"},"trusted":true},"outputs":[],"source":["# Ensure all features have the same length\n","max_length = max([len(x) for x in X])\n","X_padded = np.array([np.pad(x, (0, max_length - len(x)), mode='constant') for x in X])\n","\n","print(f\"Padded features shape: {X_padded.shape}\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-22T01:24:41.769509Z","iopub.status.busy":"2024-05-22T01:24:41.769179Z","iopub.status.idle":"2024-05-22T01:24:42.336236Z","shell.execute_reply":"2024-05-22T01:24:42.335427Z","shell.execute_reply.started":"2024-05-22T01:24:41.769483Z"},"trusted":true},"outputs":[],"source":["from sklearn.preprocessing import LabelEncoder\n","\n","label_encoder = LabelEncoder()\n","Y_encoded = label_encoder.fit_transform(Y)\n","visualizer = YBPCA(scale=True, projection=2, classes=np.unique(Y_encoded))\n","visualizer.fit_transform(X, Y_encoded)\n","visualizer.show()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-22T01:24:49.242625Z","iopub.status.busy":"2024-05-22T01:24:49.241804Z","iopub.status.idle":"2024-05-22T01:24:51.342757Z","shell.execute_reply":"2024-05-22T01:24:51.341742Z","shell.execute_reply.started":"2024-05-22T01:24:49.242592Z"},"trusted":true},"outputs":[],"source":["fig = px.box(data_frame=pd.DataFrame(X_padded), points=\"all\", title=\"Distribution of Features\")\n","fig.update_layout(title_text='Distribution of Features', title_x=0.5, \n","                  xaxis_title='Feature Index', yaxis_title='Value')\n","\n","fig.show()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-22T01:33:16.056874Z","iopub.status.busy":"2024-05-22T01:33:16.056513Z","iopub.status.idle":"2024-05-22T01:33:17.449543Z","shell.execute_reply":"2024-05-22T01:33:17.448569Z","shell.execute_reply.started":"2024-05-22T01:33:16.056844Z"},"trusted":true},"outputs":[],"source":["Features = pd.DataFrame(X)\n","Features['labels'] = Y\n","Features.to_csv('features.csv', index=False)\n","Features.head()"]},{"cell_type":"markdown","metadata":{},"source":["## Data Preparation\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-22T01:40:29.313771Z","iopub.status.busy":"2024-05-22T01:40:29.313028Z","iopub.status.idle":"2024-05-22T01:40:29.319632Z","shell.execute_reply":"2024-05-22T01:40:29.318780Z","shell.execute_reply.started":"2024-05-22T01:40:29.313739Z"},"trusted":true},"outputs":[],"source":["X = Features.iloc[: ,:-1].values\n","Y = Features['labels'].values\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# As this is a multiclass classification problem onehotencoding our Y.\n","encoder = OneHotEncoder()\n","Y = encoder.fit_transform(np.array(Y).reshape(-1,1)).toarray()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-22T01:42:35.489549Z","iopub.status.busy":"2024-05-22T01:42:35.488812Z","iopub.status.idle":"2024-05-22T01:42:35.499457Z","shell.execute_reply":"2024-05-22T01:42:35.498535Z","shell.execute_reply.started":"2024-05-22T01:42:35.489511Z"},"trusted":true},"outputs":[],"source":["# splitting data\n","x_train, x_test, y_train, y_test = train_test_split(X, Y, random_state=0, shuffle=True)\n","x_train.shape, y_train.shape, x_test.shape, y_test.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-22T01:42:49.378703Z","iopub.status.busy":"2024-05-22T01:42:49.378333Z","iopub.status.idle":"2024-05-22T01:42:49.393587Z","shell.execute_reply":"2024-05-22T01:42:49.392714Z","shell.execute_reply.started":"2024-05-22T01:42:49.378672Z"},"trusted":true},"outputs":[],"source":["# scaling our data with sklearn's Standard scaler\n","scaler = StandardScaler()\n","x_train = scaler.fit_transform(x_train)\n","x_test = scaler.transform(x_test)\n","x_train.shape, y_train.shape, x_test.shape, y_test.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-22T01:43:07.598918Z","iopub.status.busy":"2024-05-22T01:43:07.598549Z","iopub.status.idle":"2024-05-22T01:43:07.605755Z","shell.execute_reply":"2024-05-22T01:43:07.604845Z","shell.execute_reply.started":"2024-05-22T01:43:07.598877Z"},"trusted":true},"outputs":[],"source":["# making our data compatible to model.\n","x_train = np.expand_dims(x_train, axis=2)\n","x_test = np.expand_dims(x_test, axis=2)\n","x_train.shape, y_train.shape, x_test.shape, y_test.shape"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import pickle\n","# Saving The Encoder\n","with open('speechmodel_tokenizer.pickle', 'wb') as handle:\n","    pickle.dump(encoder, handle, protocol=pickle.HIGHEST_PROTOCOL)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import tensorflow as tf\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","\n","# Save the tokenizer to a file\n","with open('textmodel_tokenizer.pickle', 'wb') as handle:\n","    pickle.dump(Tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n","# Save the Encoder to a file\n","with open('text_Encoder.pickle', 'wb') as hander:\n","    pickle.dump(encoder, hander, protocol=pickle.HIGHEST_PROTOCOL)"]},{"cell_type":"markdown","metadata":{},"source":["## Modelling"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-22T02:08:44.318012Z","iopub.status.busy":"2024-05-22T02:08:44.316806Z","iopub.status.idle":"2024-05-22T02:08:44.527157Z","shell.execute_reply":"2024-05-22T02:08:44.525957Z","shell.execute_reply.started":"2024-05-22T02:08:44.317965Z"},"trusted":true},"outputs":[],"source":["from keras.models import Sequential\n","from keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, Dropout\n","from keras.callbacks import ReduceLROnPlateau\n","import numpy as np\n","\n","# Define the model\n","model = Sequential()\n","model.add(Conv1D(256, kernel_size=5, strides=1, padding='same', activation='relu', input_shape=(x_train.shape[1], 1)))\n","model.add(MaxPooling1D(pool_size=5, strides=2, padding='same'))\n","\n","model.add(Conv1D(256, kernel_size=5, strides=1, padding='same', activation='relu'))\n","model.add(MaxPooling1D(pool_size=5, strides=2, padding='same'))\n","\n","model.add(Conv1D(128, kernel_size=5, strides=1, padding='same', activation='relu'))\n","model.add(MaxPooling1D(pool_size=5, strides=2, padding='same'))\n","model.add(Dropout(0.2))\n","\n","model.add(Conv1D(64, kernel_size=5, strides=1, padding='same', activation='relu'))\n","model.add(MaxPooling1D(pool_size=5, strides=2, padding='same'))\n","\n","model.add(Flatten())\n","model.add(Dense(units=32, activation='relu'))\n","model.add(Dropout(0.3))\n","\n","# Adjust the output layer to have 7 units for 7 classes\n","model.add(Dense(units=8, activation='softmax'))\n","\n","model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n","\n","# Define the learning rate reducer callback\n","rlrp = ReduceLROnPlateau(monitor='loss', factor=0.4, verbose=0, patience=2, min_lr=0.0000001)\n","\n","# Ensure y_train and y_test are one-hot encoded\n","from keras.utils import to_categorical\n","y_train_one_hot = to_categorical(y_train)\n","y_test_one_hot = to_categorical(y_test)\n","\n","# Fit the model\n","history = model.fit(x_train, y_train_one_hot, batch_size=64, epochs=50, validation_data=(x_test, y_test_one_hot), callbacks=[rlrp])\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-22T01:56:52.367062Z","iopub.status.busy":"2024-05-22T01:56:52.366214Z","iopub.status.idle":"2024-05-22T01:56:52.586516Z","shell.execute_reply":"2024-05-22T01:56:52.585236Z","shell.execute_reply.started":"2024-05-22T01:56:52.367028Z"},"trusted":true},"outputs":[],"source":["# Define the learning rate reducer callback\n","rlrp = ReduceLROnPlateau(monitor='loss', factor=0.4, verbose=0, patience=2, min_lr=0.0000001)\n","\n","# Fit the model\n","history = model.fit(x_train, y_train, batch_size=64, epochs=100, validation_data=(x_test, y_test), callbacks=[rlrp])\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-21T21:37:18.579189Z","iopub.status.busy":"2024-05-21T21:37:18.578455Z","iopub.status.idle":"2024-05-21T21:37:18.625212Z","shell.execute_reply":"2024-05-21T21:37:18.624340Z","shell.execute_reply.started":"2024-05-21T21:37:18.579142Z"},"trusted":true},"outputs":[],"source":["model.save('Speech_Emotion_model.h5')\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-21T21:37:32.218860Z","iopub.status.busy":"2024-05-21T21:37:32.218491Z","iopub.status.idle":"2024-05-21T21:37:34.871092Z","shell.execute_reply":"2024-05-21T21:37:34.869538Z","shell.execute_reply.started":"2024-05-21T21:37:32.218834Z"},"trusted":true},"outputs":[],"source":["print(\"Accuracy of our model on test data : \" , model.evaluate(x_test,y_test)[1]*100 , \"%\")\n","\n","epochs = [i for i in range(50)]\n","fig , ax = plt.subplots(1,2)\n","train_acc = history.history['accuracy']\n","train_loss = history.history['loss']\n","test_acc = history.history['val_accuracy']\n","test_loss = history.history['val_loss']\n","\n","fig.set_size_inches(20,6)\n","ax[0].plot(epochs , train_loss , label = 'Training Loss')\n","ax[0].plot(epochs , test_loss , label = 'Testing Loss')\n","ax[0].set_title('Training & Testing Loss')\n","ax[0].legend()\n","ax[0].set_xlabel(\"Epochs\")\n","\n","ax[1].plot(epochs , train_acc , label = 'Training Accuracy')\n","ax[1].plot(epochs , test_acc , label = 'Testing Accuracy')\n","ax[1].set_title('Training & Testing Accuracy')\n","ax[1].legend()\n","ax[1].set_xlabel(\"Epochs\")\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# predicting on test data.\n","pred_test = model.predict(x_test)\n","y_pred = encoder.inverse_transform(pred_test)\n","\n","y_test = encoder.inverse_transform(y_test)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-22T00:36:16.097824Z","iopub.status.busy":"2024-05-22T00:36:16.097252Z","iopub.status.idle":"2024-05-22T00:36:16.443666Z","shell.execute_reply":"2024-05-22T00:36:16.442245Z","shell.execute_reply.started":"2024-05-22T00:36:16.097790Z"},"trusted":true},"outputs":[],"source":["df = pd.DataFrame(columns=['Predicted Labels', 'Actual Labels'])\n","df['Predicted Labels'] = y_pred.flatten()\n","df['Actual Labels'] = y_test.flatten()\n","\n","df.head(10)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["cm = confusion_matrix(y_test, y_pred)\n","plt.figure(figsize = (12, 10))\n","cm = pd.DataFrame(cm , index = [i for i in encoder.categories_] , columns = [i for i in encoder.categories_])\n","sns.heatmap(cm, linecolor='white', cmap='Blues', linewidth=1, annot=True, fmt='')\n","plt.title('Confusion Matrix', size=20)\n","plt.xlabel('Predicted Labels', size=14)\n","plt.ylabel('Actual Labels', size=14)\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["print(classification_report(y_test, y_pred))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"datasetId":107620,"sourceId":256618,"sourceType":"datasetVersion"},{"datasetId":338555,"sourceId":671851,"sourceType":"datasetVersion"}],"dockerImageVersionId":30699,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.3"}},"nbformat":4,"nbformat_minor":4}
